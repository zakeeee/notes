# 输入输出

[TOC]

## IO 模式

### 阻塞 IO

Linux 默认情况下 Socket IO 是阻塞 IO。

![](_v_images/20190826090348545_31322.png)

阻塞 IO 模式下，当用户进程调用了 recvfrom 这个系统调用，内核就开始 IO 的第一阶段准备数据。数据被拷贝到内核缓冲区是需要等待的。当数据准备完成后，会进入 IO 的第二阶段，内核把数据拷贝到用户进程缓冲区，并返回结果，这时用户进程才解除阻塞。

阻塞 IO 模式下，IO 的两个阶段都是阻塞的。一旦一个 IO 阻塞起来，就无法做其他事情，效率低。

### 非阻塞 IO

Linux 下可以设置 Socket 使其成为非阻塞的。

![](_v_images/20190826092033424_19038.png)

非阻塞 IO 模式下，当用户进程执行 recvfrom 时，如果内核中数据还没准备好，内核不会阻塞用户进程，而是会返回一个错误。从用户进程来说，它并不需要等待，而是马上就可以得到一个结果。当用户进程判断这是一个错误信息时，就知道数据还没准备好。用户进程不断执行这样的操作，直到数据准备完成，内核就将数据拷贝到用户进程缓冲区。

非阻塞 IO 模式下，用户进程需要不断查询直到数据准备完毕。这个过程很浪费 CPU 资源。

### 多路复用 IO

多路复用 IO 模式就是 select，poll，epoll 等（kqueue 是 bsd 系下的）。在多路复用 IO 模式下，单个进程可以处理多个 IO。多路复用 IO 模式在调用 select 时是阻塞的，只有等待 select 返回才能继续，因此还是属于阻塞的 IO 类型。

![](_v_images/20190826092040289_22570.png)

下面以 select 函数为例，多路复用 IO 模式下，用户进程调用 select 时会阻塞，select 会轮询其所负责的所有 Socket，当某个 Socket 有数据到达，select 就会返回，这时用户进程再调用 recvfrom 操作，内核会将数据从内核缓冲区拷贝到用户进程缓冲区。

所以多路复用 IO 其实是通过一种机制使得一个进程可以同时等待多个 fd（文件描述符），当其中某个 fd 变为可读时，select 函数就会返回。

需要注意的是，对于单个 IO 来说，这种方式比阻塞 IO 效率还要低，因为它进行了两次系统调用。多路复用 IO 并不能使得单个连接的处理更快，其优势是可以同时处理多个连接，当连接数多的时候，不需要花大量时间在等待某一个连接的数据准备上。

### 异步 IO

上面三种 IO 模式都是同步的。同步需要主动等待消息通知，而异步则是被动接收消息通知，通过回调、通知、状态等方式来被动获取消息。Linux 下的异步 IO 其实用的很少。异步 IO 同样不会阻塞进程。

![](_v_images/20190826092047298_3437.png)

异步 IO 模式下，用户进程调用 aio_read 后，内核会立即返回，不会阻塞用户进程，然后内核会等待数据准备完成，然后将数据拷贝到用户进程缓冲区，当操作完成后，内核会向用户进程发送一个信号，告诉用户进程数据拷贝完成，可以对数据进行操作了。

### 信号驱动 IO

信号驱动 IO 在实际中很少使用。

![](_v_images/20190826095859154_23884.png)

信号驱动 IO 与异步 IO 的区别在于，信号驱动 IO 是等待数据完成后，内核发送一个信号给用户进程，然后用户进程使用 recvfrom 去读取数据。而异步 IO 是等待数据和数据拷贝到用户进程缓冲区都完成后，内核发送一个信号给用户进程，用户进程可以直接处理数据。

## select，poll 和 epoll

select，poll 和 epoll 都是多路复用 IO 的实现。

### select

```c
int select(int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```

select 函数监视的 fd 分3类，分别是 writefds，readfds 和 exceptfds。调用后 select 函数会阻塞，直到有 fd 就绪（有数据可读、可写、或者有except），或者超时（timeout 指定等待时间，如果立即返回设为 null 即可），函数返回。当 select 函数返回后，可以 通过遍历 fd_set，来找到就绪的描述符。

---

select 的执行流程如下：

1. 使用 `copy_from_user` 从用户空间拷贝 fd_set 到内核空间。
2. 注册回调函数 __pollwait。
3. 遍历所有 fd，调用其对应的 poll 方法。对于 socket，这个 poll 方法是 sock_poll，sock_poll 根据情况会调用到 tcp_poll，udp_poll 或者 datagram_poll。
4. 以 tcp_poll 为例，其核心实现就是 __pollwait，也就是上面注册的回调函数。
5. __pollwait 的主要工作就是把 current（当前进程）挂到设备的等待队列中，不同的设备有不同的等待队列，对于 tcp_poll 来说，其等待队列是 sk->sk_sleep（注意把进程挂到等待队列中并不代表进程已经睡眠了）。在设备收到一条消息（网络设备）或填写完文件数据（磁盘设备）后，会唤醒设备等待队列上睡眠的进程，这时 current 便被唤醒了。
6. poll 方法返回时会返回一个描述读写操作是否就绪的 mask 掩码，根据这个 mask 掩码给 fd_set 赋值。
7. 如果遍历完所有的 fd，还没有返回一个可读写的 mask 掩码，则会调用 schedule_timeout 是调用 select 的进程（也就是 current）进入睡眠。当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的进程。如果超过一定的超时时间（schedule_timeout 指定），还是没人唤醒，则调用 select 的进程会重新被唤醒获得 CPU，进而重新遍历 fd，判断有没有就绪的 fd。
8. 把 fd_set 从内核空间拷贝到用户空间。

---

select 目前几乎在所有的平台上支持，其良好跨平台支持是它的一个优点。

select 的几个缺点：

- 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大。
- 同时每次调用 select 都需要在内核遍历传递进来的所有 fd，这个开销在 fd 很多时也很大。
- select 支持的 fd 数量太小了，默认是 1024。

### poll

```c
int poll(struct pollfd *fds, unsigned int nfds, int timeout);
```

不同于 select 使用三个位图来表示三个 fdset 的方式，poll 只使用一个 pollfd 的指针实现。

```c
struct pollfd {
    int fd; /* file descriptor */
    short events; /* requested events to watch */
    short revents; /* returned events witnessed */
};
```

pollfd 结构包含了要监视的 event 和发生的 event，不再使用 select“参数-值”传递的方式。同时，pollfd 并没有最大数量限制，但是数量过大后性能也是会下降。 和 select 函数一样，poll 返回后，需要轮询 pollfd 来获取就绪的描述符。

从上面看，select 和 poll 都需要在返回后，通过遍历 fd 来获取已经就绪的 socket。事实上，同时连接的大量客户端在一时刻可能只有很少的处于就绪状态，因此随着监视的描述符数量的增长，其效率也会线性下降。

### epoll

epoll 提供了三个函数，epoll_create，epoll_ctl 和 epoll_wait：

- epoll_create：用来创建一个 epoll 句柄。
- epoll_ctl：用来注册要监听的事件类型。
- epoll_wait：用来等待事件的产生。

```c
int epoll_create(int size)；// size 用来告诉内核这个监听的数目一共有多大
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event)；
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

---

**epoll 是如何改进 select 和 poll 的缺点的**

对于第一个缺点，epoll 的解决方案在 epoll_ctl 函数中。每次注册新的事件到 epoll 句柄中时（在 epoll_ctl 中指定 EPOLL_CTL_ADD），会把所有的 fd 拷贝进内核，而不是在 epoll_wait 的时候重复拷贝。epoll 保证了每个 fd 在整个过程中只会拷贝一次。

对于第二个缺点，epoll 的解决方案不像 select 或 poll 一样每次都把 current 轮流加入 fd 对应的设备等待队列中，而只在 epoll_ctl 时把 current 挂一遍（这一遍必不可少）并为每个 fd 指定一个回调函数，当设备就绪，唤醒等待队列上的等待者时，就会调用这个回调函数，而这个回调函数会把就绪的 fd 加入一个就绪链表）。epoll_wait 的工作实际上就是在这个就绪链表中查看有没有就绪的 fd（利用 schedule_timeout() 实现睡一会，判断一会的效果，和 select 实现中的第 7 步是类似的）。

对于第三个缺点，epoll 没有这个限制，它所支持的 fd 上限是最大可以打开文件的数目，这个数字一般远大于 2048（比如在 1GB 内存的机器上大约是 10 万左右），具体数目可以使用 `cat /proc/sys/fs/file-max` 察看，一般来说这个数目和系统内存关系很大。